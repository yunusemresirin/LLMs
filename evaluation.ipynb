{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## evaluation functions for geoparsers - takes in a json file containing\n",
    "##  results from different models, which is already formatted and only contains resulting location that HAVE a coordinate\n",
    "\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    # Function to calculate distance between two coordinates\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1 = radians(coord1[0]), radians(coord1[1])\n",
    "    lat2, lon2 = radians(coord2[0]), radians(coord2[1])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = 6371 * c  # Radius of Earth in kilometers\n",
    "    return distance\n",
    "\n",
    "def compute_precision_recall_f1(instances, _truth, _pred):\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    matched_coordinates = []\n",
    "\n",
    "    for instance in instances:\n",
    "        ground_truth = instance[_truth]\n",
    "        predicted = instance[_pred]\n",
    "        matched_ground_truth = set()  # To keep track of matched ground truth elements\n",
    "        \n",
    "        true_positives = 0\n",
    "        for pred_key, pred_coord in predicted.items():\n",
    "            matched = False\n",
    "            for gt_key, gt_coord in ground_truth.items():\n",
    "                if pred_key.lower() in gt_key.lower() or gt_key.lower() in pred_key.lower():\n",
    "                    if gt_key not in matched_ground_truth:  # Ensure we don't count the same ground truth multiple times\n",
    "                        true_positives += 1\n",
    "                        matched_ground_truth.add(gt_key)\n",
    "                        matched_coordinates.append((pred_coord, gt_coord))\n",
    "                        matched = True\n",
    "                        break\n",
    "            \n",
    "            # False positives are elements in predicted that did not match any ground truth element\n",
    "            if not matched:\n",
    "                total_false_positives += 1\n",
    "        \n",
    "        # False negatives are ground truth elements that did not match any predicted element\n",
    "        total_false_negatives += len(ground_truth) - len(matched_ground_truth)\n",
    "        total_true_positives += true_positives\n",
    "    \n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score, matched_coordinates\n",
    "\n",
    "def calculate_A_at_k(matched_coordinates, k):\n",
    "    # Function to calculate accuracy at k (A@k)\n",
    "    correct_matches = 0\n",
    "    for pred_coord, truth_coord in matched_coordinates:\n",
    "        if calculate_distance(pred_coord, truth_coord) <= k:\n",
    "            correct_matches += 1\n",
    "\n",
    "    accuracy_at_k = (correct_matches / len(matched_coordinates)) * 100 if matched_coordinates else 0\n",
    "    return accuracy_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is how the above code is called\n",
    "import json\n",
    "import pprint\n",
    "with open(\"web_corpora_geoparsed_llm.json\", 'r') as json_file:\n",
    "        instances = json.load(json_file)\n",
    "#print([k for k,v in instances[0].items()])\n",
    "res = []\n",
    "en = ['locations','camcoder_en', 'clavin_en', 'spacy_dict_en','spacy_gn_en','mordecai_en', 'llm_en', 'edinburgh_en']\n",
    "de = ['locations_de', 'camcoder_de', 'clavin_de', 'spacy_dict_de', 'spacy_gn_de', 'mordecai_de', 'llm_de' ,'edinburgh_de']\n",
    "ro = ['locations_ro', 'camcoder_ro', 'clavin_ro', 'spacy_dict_ro', 'spacy_gn_ro', 'mordecai_ro', 'llm_ro', 'edinburgh_ro']\n",
    "for item in en:\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1_score, matched_coordinates = compute_precision_recall_f1(instances, \"locations\", item)\n",
    "        res.append({\n",
    "                \"model\" : item,\n",
    "                \"Precision\": round(precision, 2),\n",
    "                \"Recall\": round(recall, 2),\n",
    "                \"F1 Score\": round(f1_score, 2),\n",
    "                \"A@161\": round(calculate_A_at_k(matched_coordinates, 161),2),\n",
    "                \"A@10\": round(calculate_A_at_k(matched_coordinates, 10),2)\n",
    "        })\n",
    "for item in de:\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1_score, matched_coordinates = compute_precision_recall_f1(instances, \"locations_de\", item)\n",
    "        res.append( {\n",
    "                \"model\" : item,\n",
    "                \"Precision\": round(precision, 2),\n",
    "                \"Recall\": round(recall, 2),\n",
    "                \"F1 Score\": round(f1_score, 2),\n",
    "                \"A@161\": round(calculate_A_at_k(matched_coordinates, 161),2),\n",
    "                \"A@10\": round(calculate_A_at_k(matched_coordinates, 10),2)\n",
    "        })\n",
    "\n",
    "for item in ro:\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1_score, matched_coordinates = compute_precision_recall_f1(instances, \"locations_ro\", item)\n",
    "        res.append({\n",
    "                \"model\" : item,\n",
    "                \"Precision\": round(precision, 2),\n",
    "                \"Recall\": round(recall, 2),\n",
    "                \"F1 Score\": round(f1_score, 2),\n",
    "                \"A@161\": round(calculate_A_at_k(matched_coordinates, 161),2),\n",
    "                \"A@10\": round(calculate_A_at_k(matched_coordinates, 10),2)\n",
    "        })\n",
    "# with open(\"xx.json\", 'w') as json_file:\n",
    "#         json.dump(res, json_file, indent=4)\n",
    "\n",
    "        # print(\"Precision:\", precision)\n",
    "        # print(\"Recall:\", recall)\n",
    "        # print(\"F1 Score:\", f1_score)\n",
    "\n",
    "        # # Calculate A@161 and A@10\n",
    "        # accuracy_at_161 = calculate_A_at_k(matched_coordinates, 161)\n",
    "        # accuracy_at_10 = calculate_A_at_k(matched_coordinates, 10)\n",
    "        # print(\"A@161:\", accuracy_at_161)\n",
    "        # print(\"A@10:\", accuracy_at_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NER evaluation - same strategy as above, I ran NER experiments separately and\n",
    "# there we of course do not retrieve coordinates hence everything predicted is taken into account\n",
    "\n",
    "def compute_precision_recall_f1(ground_truths, predicted_lists):\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    for key in ground_truths:\n",
    "        ground_truth = ground_truths[key]\n",
    "        predicted = predicted_lists[key]\n",
    "        \n",
    "        true_positives = 0\n",
    "        matched_ground_truth = set()  # To keep track of matched ground truth elements\n",
    "        \n",
    "        for pred in predicted:\n",
    "            matched = False\n",
    "            for gt in ground_truth:\n",
    "                if pred.lower() in gt.lower() or gt.lower() in pred.lower():\n",
    "                    if gt not in matched_ground_truth:  # Ensure we don't count the same ground truth multiple times\n",
    "                        true_positives += 1\n",
    "                        matched_ground_truth.add(gt)\n",
    "                        matched = True\n",
    "                        break\n",
    "            \n",
    "            # False positives are elements in predicted that did not match any ground truth element\n",
    "            if not matched:\n",
    "                total_false_positives += 1\n",
    "        \n",
    "        # False negatives are ground truth elements that did not match any predicted element\n",
    "        total_false_negatives += len(ground_truth) - len(matched_ground_truth)\n",
    "        total_true_positives += true_positives\n",
    "    \n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "def get_eval_res_new (data_file, key, x):\n",
    "    with open(data_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    result = []\n",
    "    for model_ in x:  \n",
    "        print(model_)\n",
    "        ground_truths =  {item[\"id\"]: item[key] for item in data}\n",
    "        predicted_lists = {item[\"id\"]: item[model_] for item in data}\n",
    "\n",
    "        precision, recall, f1_score = compute_precision_recall_f1(ground_truths, predicted_lists)\n",
    "        \n",
    "        result.append({\"model\":model_.replace(\"pred_\",\"\"),\n",
    "                       \"precision\": round(precision,2),\n",
    "                       \"recall\": round(recall,2),\n",
    "                       \"f1_score\": round(f1_score,2),\n",
    "                      })\n",
    "        \n",
    "    # with open(\"./NER_final/\"+x[0]+\"_web_corpora_NER.json\", 'w') as json_file:\n",
    "    #     json.dump(result, json_file, indent=4)\n",
    "\n",
    "en = [\"en\",'spacy_xx_en', 'spacy_en_en', 'gliner_en', 'stanNER_en', 'flair_en',  'gazpne_en']\n",
    "de = [\"de\",'spacy_xx_de', 'spacy_en_de', 'gliner_de', 'stanNER_de', 'flair_de',  'gazpne_de']\n",
    "ro = [\"ro\",'spacy_xx_ro', 'spacy_en_ro', 'gliner_ro', 'stanNER_ro', 'flair_ro',  'gazpne_ro']\n",
    "\n",
    "get_eval_res_new(\"./NER_final/web_corpora_all_NER.json\", \"en\", en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
